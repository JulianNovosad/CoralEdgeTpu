diff --git a/src/inference.cpp b/src/inference.cpp
index acae3e6..d0de42a 100644
--- a/src/inference.cpp
+++ b/src/inference.cpp
@@ -1,21 +1,53 @@
 #include "inference.h"
+#include "util_logging.h"
 #include <iostream>
-#include <algorithm> // For std::sort
-#include <cmath>     // For std::abs
+#include <stdexcept>
+#include <algorithm>
+#include <cmath>
 
-// Edge TPU delegate C API functions
+// Edge TPU delegate C API functions - Adjusted signature based on previous iteration backup
 extern "C" {
-    TfLiteDelegate* tflite_plugin_create_delegate(char** options_keys, char** options_values, size_t num_options);
+    TfLiteDelegate* tflite_plugin_create_delegate(const void* options);
     void tflite_plugin_destroy_delegate(TfLiteDelegate* delegate);
 }
 
-InferenceEngine::InferenceEngine(const std::string& model_path, ThreadSafeQueue& input_queue, DetectionQueue& output_queue, int num_threads)
-    : model_path_(model_path), input_queue_(input_queue), output_queue_(output_queue), num_threads_(num_threads) {
+InferenceEngine::InferenceEngine(const std::string& model_path, ImageQueue& input_queue, UdpQueue& udp_output_queue, int num_threads)
+    : model_path_(model_path), input_queue_(input_queue), udp_output_queue_(udp_output_queue), num_threads_(num_threads) {
 
     model_ = tflite::FlatBufferModel::BuildFromFile(model_path_.c_str());
     if (!model_) {
-        std::cerr << "Failed to load model: " << model_path_ << std::endl;
-        // Handle error appropriately, e.g., throw exception
+        throw std::runtime_error("Failed to load model: " + model_path_ + ". Please ensure the model path is correct and the file exists.");
+    }
+
+    std::unique_ptr<tflite::Interpreter> interpreter;
+    tflite::InterpreterBuilder(*model_, resolver_)(&interpreter);
+    if (!interpreter) {
+        throw std::runtime_error("Failed to create temporary interpreter for model inspection.");
+    }
+    
+    // Check if the delegate can be created. This is a pre-check before multi-threading.
+    TfLiteDelegate* test_delegate = tflite_plugin_create_delegate(nullptr);
+    if (!test_delegate) {
+        throw std::runtime_error("Failed to create Edge TPU delegate during initialization. Ensure Edge TPU drivers are installed and device is connected.");
+    }
+    // Destroy the test delegate immediately as it's not owned by the interpreter yet
+    tflite_plugin_destroy_delegate(test_delegate);
+
+
+    int input_tensor_idx = interpreter->inputs()[0];
+    TfLiteTensor* input_tensor = interpreter->tensor(input_tensor_idx);
+    
+    if (input_tensor->dims->size < 4) {
+        throw std::runtime_error("Model input tensor has fewer than 4 dimensions, which is not supported.");
+    }
+    input_height_ = input_tensor->dims->data[1];
+    input_width_ = input_tensor->dims->data[2];
+    input_channels_ = input_tensor->dims->data[3];
+    
+    LOG_INFO("Model Input Dimensions: " + std::to_string(input_width_) + "x" + std::to_string(input_height_) + "x" + std::to_string(input_channels_));
+
+    if (input_channels_ != 3) {
+        throw std::runtime_error("Model expects " + std::to_string(input_channels_) + " channels, but this application is hardcoded for 3 (RGB).");
     }
 }
 
@@ -25,185 +57,178 @@ InferenceEngine::~InferenceEngine() {
 
 bool InferenceEngine::start() {
     if (running_) {
-        std::cerr << "InferenceEngine is already running." << std::endl;
+        LOG_ERROR("InferenceEngine is already running.");
         return false;
     }
-
     if (!model_) {
-        std::cerr << "Model not loaded, cannot start inference engine." << std::endl;
+        LOG_ERROR("Model not loaded, cannot start inference engine.");
         return false;
     }
 
     running_ = true;
-    for (int i = 0; i < num_threads_; ++i) {
-        // Create an interpreter for each thread
-        std::unique_ptr<tflite::Interpreter> interpreter = create_interpreter();
-        if (!interpreter) {
-            std::cerr << "Failed to create interpreter for worker " << i << std::endl;
-            running_ = false;
-            return false;
-        }
-        std::lock_guard<std::mutex> lock(interpreter_pool_mutex_);
-        interpreter_pool_.push(std::move(interpreter));
+    // Also re-set the queues to running, in case they were stopped before.
+    input_queue_.set_running(true);
+    udp_output_queue_.set_running(true);
 
+    for (int i = 0; i < num_threads_; ++i) {
         worker_threads_.emplace_back(&InferenceEngine::worker_thread_func, this);
     }
 
-    std::cout << "InferenceEngine started with " << num_threads_ << " worker threads." << std::endl;
+    LOG_INFO("InferenceEngine started with " + std::to_string(num_threads_) + " worker threads.");
     return true;
 }
 
 void InferenceEngine::stop() {
-    if (!running_) {
-        return;
-    }
-    running_ = false;
-    output_queue_.set_running(false); // Signal consumers to stop
-    // Also signal the input queue to unblock the inference threads
-    input_queue_.set_running(false);
-    
-    for (std::thread& thread : worker_threads_) {
-        if (thread.joinable()) {
-            thread.join();
+    if (running_.exchange(false)) {
+        LOG_INFO("Stopping InferenceEngine...");
+        udp_output_queue_.set_running(false);
+        input_queue_.set_running(false);
+        
+        for (std::thread& thread : worker_threads_) {
+            if (thread.joinable()) {
+                thread.join();
+            }
         }
+        worker_threads_.clear();
+        LOG_INFO("InferenceEngine stopped.");
     }
-    worker_threads_.clear();
-    std::cout << "InferenceEngine stopped." << std::endl;
 }
 
 std::unique_ptr<tflite::Interpreter> InferenceEngine::create_interpreter() {
     std::unique_ptr<tflite::Interpreter> local_interpreter;
     tflite::InterpreterBuilder(*model_, resolver_)(&local_interpreter);
-
     if (!local_interpreter) {
-        std::cerr << "Failed to create interpreter." << std::endl;
+        LOG_ERROR("Failed to build interpreter.");
         return nullptr;
     }
 
-    // Attach EdgeTPU delegate
-    // Options can be passed as key-value pairs if needed
-    char* options_keys[] = {};
-    char* options_values[] = {};
-    size_t num_options = 0;
-
-    TfLiteDelegate* delegate = tflite_plugin_create_delegate(options_keys, options_values, num_options);
+    // Create the Edge TPU delegate
+    TfLiteDelegate* delegate = tflite_plugin_create_delegate(nullptr);
     if (!delegate) {
-        std::cerr << "Failed to create EdgeTPU delegate." << std::endl;
+        LOG_ERROR("Failed to create EdgeTPU delegate. Ensure libedgetpu1-std is installed and device is connected.");
         return nullptr;
     }
 
     if (local_interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) {
-        std::cerr << "Failed to apply EdgeTPU delegate." << std::endl;
-        tflite_plugin_destroy_delegate(delegate);
+        LOG_ERROR("Failed to apply EdgeTPU delegate. Check if the model is compatible.");
+        tflite_plugin_destroy_delegate(delegate); // Delegate is not owned by interpreter on failure
         return nullptr;
     }
-
+    
+    // Allocate tensors *after* applying the delegate
     if (local_interpreter->AllocateTensors() != kTfLiteOk) {
-        std::cerr << "Failed to allocate tensors." << std::endl;
-        tflite_plugin_destroy_delegate(delegate);
+        LOG_ERROR("Failed to allocate tensors after applying EdgeTPU delegate.");
+        // If AllocateTensors fails *after* ModifyGraphWithDelegate succeeds, the interpreter
+        // might still own the delegate, but to be safe, we'll try to destroy it.
+        // In a robust system, error handling here can be more nuanced.
+        tflite_plugin_destroy_delegate(delegate); 
         return nullptr;
     }
-
+    
     return local_interpreter;
 }
 
 void InferenceEngine::worker_thread_func() {
-    std::unique_ptr<tflite::Interpreter> interpreter;
-
-    // Get an interpreter from the pool
-    {
-        std::lock_guard<std::mutex> lock(interpreter_pool_mutex_);
-        if (!interpreter_pool_.empty()) {
-            interpreter = std::move(interpreter_pool_.front());
-            interpreter_pool_.pop();
-        } else {
-            std::cerr << "Interpreter pool is empty for worker thread!" << std::endl;
-            return; // Should not happen if pool is sized correctly
-        }
+    std::unique_ptr<tflite::Interpreter> interpreter = create_interpreter();
+    if (!interpreter) {
+        LOG_ERROR("Worker thread failed to create interpreter. Exiting thread.");
+        return;
     }
-
+    // Add the interpreter to the pool for other workers to potentially use (though in this model, each worker creates its own).
+    // This part of the original design for an interpreter pool seems a bit off if each worker creates its own interpreter.
+    // For now, I will modify it to create the interpreter directly.
+    // The previous implementation added to the pool, then popped. This is cleaner.
+    
     ImageData input_image;
     while (running_) {
-        if (input_queue_.pop(input_image)) { // Blocking call
-            auto inference_start_time = std::chrono::high_resolution_clock::now();
+        if (input_queue_.pop(input_image)) {
+            // Check if image data size matches expected input tensor size
+            int expected_input_size = input_width_ * input_height_ * input_channels_;
+            if (input_image.data.size() != expected_input_size) {
+                 LOG_ERROR("Input image data size (" + std::to_string(input_image.data.size()) + 
+                           ") does not match expected model input size (" + std::to_string(expected_input_size) + ")");
+                 continue; // Skip this frame
+            }
 
             set_input_tensor(interpreter.get(), input_image);
 
             if (interpreter->Invoke() != kTfLiteOk) {
-                std::cerr << "Failed to invoke interpreter." << std::endl;
+                LOG_ERROR("Failed to invoke interpreter.");
                 continue;
             }
-
-            std::vector<DetectionResult> results = get_output_tensor(interpreter.get(), input_image);
-            output_queue_.push(std::move(results));
-
-            auto inference_end_time = std::chrono::high_resolution_clock::now();
-            std::chrono::duration<double, std::milli> inference_duration = inference_end_time - inference_start_time;
-            // std::cout << "Inference time: " << inference_duration.count() << " ms" << std::endl;
+            
+            // Get detection results
+            std::vector<DetectionResult> results = get_output_tensor(interpreter.get());
+            
+            // Push results directly to the UDP queue if there are any detections
+            if (!results.empty()) {
+                udp_output_queue_.push(std::move(results));
+            }
         }
     }
-
-    // Return interpreter to pool (or destroy it, if we're shutting down the pool)
-    // For now, it will be destroyed when unique_ptr goes out of scope.
-    // If we wanted to re-pool it, we'd push it back to a shared queue before exiting.
 }
 
 void InferenceEngine::set_input_tensor(tflite::Interpreter* interpreter, const ImageData& image) {
-    // Assuming the model expects a single input tensor of type kTfLiteUInt8
-    // and layout (1, height, width, channels)
-    // Here we need to convert the YUV420 image to RGB or grayscale as expected by the model.
-    // For now, we'll assume the model expects a single-channel grayscale image
-    // and copy the Y plane directly, or if it expects RGB, we'd need conversion.
-
     int input_tensor_idx = interpreter->inputs()[0];
     TfLiteTensor* input_tensor = interpreter->tensor(input_tensor_idx);
 
     if (input_tensor->type != kTfLiteUInt8) {
-        std::cerr << "Input tensor type is not kTfLiteUInt8. Model might expect a different format." << std::endl;
-        // Handle error, maybe convert type
+        LOG_ERROR("Input tensor type is not kTfLiteUInt8 as expected. Current type: " + std::to_string(input_tensor->type));
+        return;
     }
-
-    if (input_tensor->bytes != image.data.size()) {
-        std::cerr << "WARNING: Input tensor size mismatch. Expected " << input_tensor->bytes
-                  << ", got " << image.data.size() << ". This will likely lead to incorrect inference results." << std::endl;
-        std::cerr << "         Image resizing needs to be implemented." << std::endl;
+    // The input_tensor->bytes should already be validated against image.data.size() before this call
+    // in worker_thread_func.
+
+    // The image from CameraCapture is BGR, but the model expects RGB. Swap channels.
+    uint8_t* tensor_data = interpreter->typed_input_tensor<uint8_t>(0);
+    const uint8_t* image_data = image.data.data();
+    int num_pixels = input_width_ * input_height_;
+
+    for (int i = 0; i < num_pixels; ++i) {
+        tensor_data[i * 3 + 0] = image_data[i * 3 + 2]; // R
+        tensor_data[i * 3 + 1] = image_data[i * 3 + 1]; // G
+        tensor_data[i * 3 + 2] = image_data[i * 3 + 0]; // B
     }
-
-    // Copy image data to input tensor
-    // This will likely cause a buffer overflow if the image is larger than the input tensor.
-    // For now, we'll just copy the first `input_tensor->bytes` bytes.
-    std::copy(image.data.begin(), image.data.begin() + input_tensor->bytes, interpreter->typed_input_tensor<uint8_t>(0));
 }
 
-std::vector<DetectionResult> InferenceEngine::get_output_tensor(tflite::Interpreter* interpreter, const ImageData& image) {
+std::vector<DetectionResult> InferenceEngine::get_output_tensor(tflite::Interpreter* interpreter) {
     std::vector<DetectionResult> results;
+    // Assuming output tensor structure for SSD MobileNet:
+    // 0: detection_boxes (float, [1, num_detections, 4])
+    // 1: detection_classes (float, [1, num_detections])
+    // 2: detection_scores (float, [1, num_detections])
+    // 3: num_detections (float, [1])
+
+    // Get output tensor details
+    // Ensure all tensors are present and have expected types
+    if (interpreter->outputs().size() < 4) {
+        LOG_ERROR("Model does not have expected number of output tensors (expected 4).");
+        return results;
+    }
 
-    // Assuming a common detection model output format:
-    // Output 0: Detection boxes (1, num_boxes, 4) - [ymin, xmin, ymax, xmax]
-    // Output 1: Detection classes (1, num_boxes)
-    // Output 2: Detection scores (1, num_boxes)
-    // Output 3: Number of detections (1)
+    const float* detection_boxes = interpreter->typed_output_tensor<float>(0);
+    const float* detection_classes = interpreter->typed_output_tensor<float>(1);
+    const float* detection_scores = interpreter->typed_output_tensor<float>(2);
+    // The num_detections tensor is usually a float that needs to be cast to int
+    const int num_detections = static_cast<int>(*interpreter->typed_output_tensor<float>(3));
 
-    int num_detections = static_cast<int>(*interpreter->typed_output_tensor<float>(3));
-    float* detection_boxes = interpreter->typed_output_tensor<float>(0);
-    float* detection_classes = interpreter->typed_output_tensor<float>(1);
-    float* detection_scores = interpreter->typed_output_tensor<float>(2);
+    auto timestamp = std::chrono::high_resolution_clock::now();
 
     for (int i = 0; i < num_detections; ++i) {
-        if (detection_scores[i] > 0.5) { // Threshold for detections
+        if (detection_scores[i] > 0.5) { // Confidence threshold
             DetectionResult res;
             res.class_id = static_cast<int>(detection_classes[i]);
             res.score = detection_scores[i];
+            res.timestamp = timestamp;
 
             // Bounding box coordinates are normalized [0, 1]
-            // Convert to absolute pixel coordinates
-            res.ymin = detection_boxes[i * 4] * image.height;
-            res.xmin = detection_boxes[i * 4 + 1] * image.width;
-            res.ymax = detection_boxes[i * 4 + 2] * image.height;
-            res.xmax = detection_boxes[i * 4 + 3] * image.width;
-            res.timestamp = image.timestamp;
+            // Order is [ymin, xmin, ymax, xmax]
+            res.ymin = detection_boxes[i * 4 + 0] * input_height_;
+            res.xmin = detection_boxes[i * 4 + 1] * input_width_;
+            res.ymax = detection_boxes[i * 4 + 2] * input_height_;
+            res.xmax = detection_boxes[i * 4 + 3] * input_width_;
             results.push_back(res);
         }
     }
     return results;
-}
+}
\ No newline at end of file
